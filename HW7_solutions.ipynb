{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7: Model Selection\n",
    "\n",
    "**Due**: Monday May 29th\n",
    "\n",
    "- **Format**: We expect students to complete the homework notebooks using Google Colab (see Discussion 1), but this is not explicitly required and you may use whatever software you would like to run notebooks. \n",
    "- **Answers**: As a general guiding policy, you should always try to make it as clear as possible what your answer to each question is, and how you arrived at your answer. Generally speaking, this will mean including all code used to generate results, outputting the actual results to the notebook, and (when necessary) including written answers to support your code.\n",
    "- **Submission**: Homeworks will be *submitted to Gradescope*, and we expect all students to do question matching on Gradescope upon submission.\n",
    "- **Late Policy**: All students are allowed 7 total slip days for the quarter, and at most 5 can be used for a single HW assignment. There will be no late credit if you have used up all your slip days. Also, your lowest HW grade will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "sns.set_style()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Identifying Bias and Variance\n",
    "\n",
    "For each of the comparisons below, identify which model has higher bias and which model has higher variance. Explain your reasoning.\n",
    "\n",
    "**Part (a)**: Suppose $y \\sim \\beta_0 + \\beta_1 x + \\epsilon$, where $\\epsilon \\sim N(0,\\sigma^2)$. \n",
    "\n",
    "We train two models on a sample of $n$ randomly generated $y_i$'s.\n",
    "1. The first model is fit via least squares and has the fitted form $\\hat{y} = \\hat{\\beta}_0$. \n",
    "2. The second model has the fitted form $\\hat{y} = y_1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: Model 1 has lower bias since it is trained on all the data versus a single data point, so it will fit the training data more closely. \n",
    "Model 1 also has lower variance since it's based on an average of all the data points, which will vary less across datasets than the value of the first data point. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (b)**: Suppose we have the same data as in part (a).\n",
    "\n",
    "We fit two models via least squares:\n",
    "1. The first model's fitted form is $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\cdots + \\hat{\\beta}_{n-1} x^{n-1}$.\n",
    "2. The second model's fitted form is $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\cdots + \\hat{\\beta}_{n-2} x^{n-2}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: Model 1 has lower bias since it will perfectly fit every point in the sample (i.e., same number of $\\hat{\\beta}$ parameters as data points).\n",
    "Model 2 has lower variance because we're afforded one additional degree of freedom in the fit, and will generalize slightly better than the first model. Note, however, that both of these models are horrendously overfitted!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (c)**: Now suppose $y \\sim \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon$, where $\\epsilon \\sim N(0,\\sigma^2)$. Further suppose $z \\sim \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\epsilon$, where $\\epsilon \\sim N(0,\\sigma^2)$.\n",
    "\n",
    "We fit two models via least squares:\n",
    "1. The first model's fitted form is $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\hat{\\beta}_3 x^3$.\n",
    "2. The second model's fitted form is $\\hat{z} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: Model 1 is unbiased since the functional form of the true data generating process is contained in the model specification.\n",
    "This is not the case for model 2.\n",
    "However, model 1 has higher variance than model 2 since it is estimating more parameters than the true DGP, and is therefore fitting to noise more strongly than model 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Bias and Variance of Linear Regression\n",
    "\n",
    "In this question, we will explore the bias-variance tradeoff for a simple linear regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (a)**: Consider simple linear regression data $(x_1, y_1), \\ldots, (x_n, y_n)$ that has been standardized so that $\\sum_i x_i = 0, \\sum_i x_i^2 = 1, \\sum_i y_i = 0, \\sum_i y_i^2 = 1$. What are the resulting least squares estimates for $\\beta_0$ and $\\beta_1$ on this standardized data?\n",
    "\n",
    "*Hint*: You may start from the formula's for $\\beta_1$ and $\\beta_0$ that are in the lecture notes, and simplify using the extra assumption that the data is standardized.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: $\\hat{\\beta}_1 = \\sum_i x_i y_i$ and $\\hat{\\beta}_0 = 0$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (b)**: Under the probabilistic model $y = \\beta_1 x + \\epsilon$, where $\\epsilon \\sim N(0, \\sigma^2)$, show that the prediction $\\hat{y} = \\hat{\\beta}_1 x$ is an unbiased predictor of $y$. \n",
    "\n",
    "*Hint:* In the standard linear regression model, $x$ is not random, only $\\epsilon$ is random."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: Using the definition of bias, $\\mathbb{E}[y - \\hat{y}] = \\mathbb{E}[\\beta_1 x + \\epsilon - \\hat{\\beta_1} x] = \\beta_1 x + 0 - x E[\\hat{\\beta_1}]$. Then \n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\hat{\\beta}_1] &= \\sum_i x_i \\mathbb{E}[y_i] \\\\\n",
    "&= \\sum_i x_i (\\beta_1 x_i + \\mathbb{E}(\\epsilon_i))\n",
    "&= \\sum_i \\beta_1 x_i^2 \\\\\n",
    "&= \\beta_1\n",
    "\\end{align*}\n",
    "because $\\sum_i x_i^2 = 1$. Thus the bias is 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (c)**: Why is it possible that $\\hat{y}$ is unbiased is part (b), and yet linear regression models can have high bias?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: Because the probabilistic model $y = \\beta_1 x + \\epsilon$ is not suitable when the actual data generating model is not linear, as is often the case in real data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Model Selection\n",
    "\n",
    "Let's return to the cars data that we looked at for HW6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = pd.read_csv(\"https://raw.githubusercontent.com/stanford-mse-125/homework/main/data/used_cars.csv\")\n",
    "honda_df = cars_df[cars_df[\"make\"] == \"Honda\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (a)**: Recall from HW6 that we used polynomial regression in order to predict price from mileage for Honda cars. \n",
    "\n",
    "Use 5-fold cross validation to estimate the out-of-sample RMSE of the following models:\n",
    "\n",
    "- Linear: $\\widehat{\\text{price}}$ = $\\beta_0$ + $\\beta_1$ $\\cdot$ $\\text{mileage}$\n",
    "- Quadratic: $\\widehat{\\text{price}}$ = $\\beta_0$ + $\\beta_1$ $\\cdot$ $\\text{mileage}$ + $\\beta_2$ $\\cdot$ $\\text{mileage}^2$\n",
    "- Cubic: $\\widehat{\\text{price}}$ = $\\beta_0$ + $\\beta_1$ $\\cdot$ $\\text{mileage}$ + $\\beta_2$ $\\cdot$ $\\text{mileage}^2$+ $\\beta_3$ $\\cdot$ $\\text{mileage}^3$\n",
    "- Quartic: $\\widehat{\\text{price}}$ = $\\beta_0$ + $\\beta_1$ $\\cdot$ $\\text{mileage}$ + $\\beta_2$ $\\cdot$ $\\text{mileage}^2$ + $\\beta_3$ $\\cdot$ $\\text{mileage}^3$ + $\\beta_4$ $\\cdot$ $\\text{mileage}^4$\n",
    "\n",
    "What model do you think is the best model for the data, based on the results of cross-validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/g5x2zyjs7jn1pbfnvwkvwh7w0000gn/T/ipykernel_58922/1310635668.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  honda_df[\"mileage**2\"] = honda_df[\"mileage\"]**2\n",
      "/var/folders/ty/g5x2zyjs7jn1pbfnvwkvwh7w0000gn/T/ipykernel_58922/1310635668.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  honda_df[\"mileage**3\"] = honda_df[\"mileage\"]**3\n",
      "/var/folders/ty/g5x2zyjs7jn1pbfnvwkvwh7w0000gn/T/ipykernel_58922/1310635668.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  honda_df[\"mileage**4\"] = honda_df[\"mileage\"]**4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "linear       2328.250723\n",
       "quadratic    2256.255468\n",
       "cubic        2259.343067\n",
       "quartic      5929.687497\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code inspired from code in Discussion 7\n",
    "# You could also do this using sklearn\n",
    "\n",
    "# Add columns for the squared and cubed mileage\n",
    "honda_df[\"mileage**2\"] = honda_df[\"mileage\"]**2\n",
    "honda_df[\"mileage**3\"] = honda_df[\"mileage\"]**3\n",
    "honda_df[\"mileage**4\"] = honda_df[\"mileage\"]**4\n",
    "\n",
    "# Split the original data (df) into three folds after shuffling the data.\n",
    "# We use three folds here since the data is small.\n",
    "folds = np.array_split(honda_df, 5)\n",
    "\n",
    "# For each held out fold, train the three linear models on the remaining\n",
    "# four folds, calculate the validation RMSE on the held out fold,\n",
    "# and store the results in a list.\n",
    "rmse_list = []\n",
    "for i in range(3):\n",
    "    # get the held out fold\n",
    "    val_df = folds[i]\n",
    "    \n",
    "    # get the other four folds\n",
    "    train_df = pd.concat(folds[:i] + folds[i+1:])\n",
    "    \n",
    "    # train the three linear models on the training set\n",
    "    linear_model = smf.ols('price ~ mileage', data=train_df).fit()\n",
    "    quadratic_model = smf.ols('price ~ mileage + I(mileage**2)', data=train_df).fit()\n",
    "    # cubic_model = smf.ols('price ~ mileage + I(mileage**2) + I(mileage**3)', data=train_df).fit()\n",
    "    # cubic_model = sm.OLS(train_df[\"price\"], sm.add_constant(train_df[[\"mileage\", \"mileage**2\", \"mileage**3\"]])).fit()\n",
    "    cubic_model = LinearRegression().fit(train_df[[\"mileage\", \"mileage**2\", \"mileage**3\"]], train_df[\"price\"])\n",
    "    quartic_model = smf.ols('price ~ mileage + I(mileage**2) + I(mileage**3) + I(mileage**4)', data=train_df).fit()\n",
    "    \n",
    "    # calculate the validation set RMSE of the three models\n",
    "    rmse_linear = np.sqrt(np.mean((val_df[\"price\"] - linear_model.predict(val_df))**2))\n",
    "    rmse_quadratic = np.sqrt(np.mean((val_df[\"price\"] - quadratic_model.predict(val_df))**2))\n",
    "    # rmse_cubic = np.sqrt(np.mean((val_df[\"price\"] - cubic_model.predict(val_df))**2))\n",
    "    rmse_cubic = np.sqrt(np.mean((val_df[\"price\"] - cubic_model.predict(val_df[[\"mileage\", \"mileage**2\", \"mileage**3\"]]))**2))\n",
    "    # rmse_cubic = np.sqrt(np.mean((val_df[\"price\"] - cubic_model.predict(sm.add_constant(val_df[[\"mileage\", \"mileage**2\", \"mileage**3\"]])))**2))\n",
    "    rmse_quartic = np.sqrt(np.mean((val_df[\"price\"] - quartic_model.predict(val_df))**2))\n",
    "    \n",
    "    # store the results in a list\n",
    "    rmse_list.append([rmse_linear, rmse_quadratic, rmse_cubic, rmse_quartic])\n",
    "\n",
    "# Finally, average the model performance across the five folds.\n",
    "# If you repeatedly run this code, you'll find that the slope+intercept model\n",
    "# tends to perform best.\n",
    "rmse_df = pd.DataFrame(rmse_list, columns=['linear', 'quadratic', 'cubic', 'quartic'])\n",
    "rmse_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07854082])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearRegression().fit(train_df[[\"mileage\"]], train_df[\"price\"]).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5653.292985850861\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>  -0.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  -44.05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 01 Jun 2023</td> <th>  Prob (F-statistic):</th>  <td>  1.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:32:39</td>     <th>  Log-Likelihood:    </th> <td> -2353.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   234</td>      <th>  AIC:               </th> <td>   4714.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   231</td>      <th>  BIC:               </th> <td>   4724.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 2.259e-05</td> <td> 7.73e-07</td> <td>   29.212</td> <td> 0.000</td> <td> 2.11e-05</td> <td> 2.41e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mileage</th>    <td>    0.6289</td> <td>    0.022</td> <td>   29.212</td> <td> 0.000</td> <td>    0.587</td> <td>    0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mileage**2</th> <td>-6.589e-06</td> <td>  3.1e-07</td> <td>  -21.227</td> <td> 0.000</td> <td> -7.2e-06</td> <td>-5.98e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mileage**3</th> <td>  1.69e-11</td> <td> 1.03e-12</td> <td>   16.449</td> <td> 0.000</td> <td> 1.49e-11</td> <td> 1.89e-11</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>15.349</td> <th>  Durbin-Watson:     </th> <td>   1.781</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  18.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.521</td> <th>  Prob(JB):          </th> <td>0.000113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.882</td> <th>  Cond. No.          </th> <td>8.31e+15</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.31e+15. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                      -0.617\n",
       "Model:                            OLS   Adj. R-squared:                 -0.631\n",
       "Method:                 Least Squares   F-statistic:                    -44.05\n",
       "Date:                Thu, 01 Jun 2023   Prob (F-statistic):               1.00\n",
       "Time:                        22:32:39   Log-Likelihood:                -2353.8\n",
       "No. Observations:                 234   AIC:                             4714.\n",
       "Df Residuals:                     231   BIC:                             4724.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       2.259e-05   7.73e-07     29.212      0.000    2.11e-05    2.41e-05\n",
       "mileage        0.6289      0.022     29.212      0.000       0.587       0.671\n",
       "mileage**2 -6.589e-06    3.1e-07    -21.227      0.000    -7.2e-06   -5.98e-06\n",
       "mileage**3   1.69e-11   1.03e-12     16.449      0.000    1.49e-11    1.89e-11\n",
       "==============================================================================\n",
       "Omnibus:                       15.349   Durbin-Watson:                   1.781\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               18.182\n",
       "Skew:                           0.521   Prob(JB):                     0.000113\n",
       "Kurtosis:                       3.882   Cond. No.                     8.31e+15\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 8.31e+15. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = smf.ols('price ~ mileage + I(mileage**2) + I(mileage**3)', data=honda_df).fit()\n",
    "model = sm.OLS(honda_df[\"price\"], sm.add_constant(honda_df[[\"mileage\", \"mileage**2\", \"mileage**3\"]])).fit()\n",
    "print(mean_squared_error(honda_df[\"price\"], model.predict(sm.add_constant(honda_df[[\"mileage\", \"mileage**2\", \"mileage**3\"]])), squared=False))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honda_df[[\"mileage\", \"mileage**2\", \"mileage**3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.17880660e-02 -2.04675498e-07  1.18177229e-12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2205.499190151544"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(honda_df[[\"mileage\", \"mileage**2\", \"mileage**3\"]], honda_df[\"price\"])\n",
    "print(model.coef_)\n",
    "mean_squared_error(honda_df[\"price\"], model.predict(honda_df[[\"mileage\", \"mileage**2\", \"mileage**3\"]]), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2316.3546127958566"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from patsy import dmatrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X = honda_df[[\"mileage\"]]\n",
    "# X_cubic = PolynomialFeatures(degree=3, include_bias=False).fit_transform(X)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    PolynomialFeatures(degree=3, include_bias=False),\n",
    "    LinearRegression(), \n",
    ")\n",
    "\n",
    "np.mean(-1 * cross_val_score(pipeline, X, honda_df[\"price\"], cv=5, scoring=\"neg_root_mean_squared_error\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic model looks like the best model. The cubic and quartic models seem to severely overfit the training data, while the quadratic model narrowly beats the linear model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (b)**: Repeat part (a), but for each linear model, also add in terms for $\\text{year}$ and $\\text{model}$ of the Honda. Do the results change? If so, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear       1748.790013\n",
       "quadratic    1703.385754\n",
       "cubic        2251.383454\n",
       "quartic      5891.859023\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code inspired from code in Discussion 7\n",
    "# You could also do this using sklearn\n",
    "\n",
    "# Split the original data (df) into three folds after shuffling the data.\n",
    "# We use three folds here since the data is small.\n",
    "folds = np.array_split(honda_df, 5)\n",
    "\n",
    "# For each held out fold, train the three linear models on the remaining\n",
    "# four folds, calculate the validation RMSE on the held out fold,\n",
    "# and store the results in a list.\n",
    "rmse_list = []\n",
    "for i in range(5):\n",
    "    # get the held out fold\n",
    "    val_df = folds[i]\n",
    "    \n",
    "    # get the other four folds\n",
    "    train_df = pd.concat(folds[:i] + folds[i+1:])\n",
    "    \n",
    "    # train the three linear models on the training set\n",
    "    linear_model = smf.ols('price ~ mileage + year + model', data=train_df).fit()\n",
    "    quadratic_model = smf.ols('price ~ mileage + year + model + I(mileage**2)', data=train_df).fit()\n",
    "    cubic_model = smf.ols('price ~ mileage + year + model + I(mileage**2) + I(mileage**3)', data=train_df).fit()\n",
    "    quartic_model = smf.ols('price ~ mileage + year + model + I(mileage**2) + I(mileage**3) + I(mileage**4)', data=train_df).fit()\n",
    "    \n",
    "    # calculate the validation set RMSE of the three models\n",
    "    rmse_linear = np.sqrt(np.mean((val_df[\"price\"] - linear_model.predict(val_df))**2))\n",
    "    rmse_quadratic = np.sqrt(np.mean((val_df[\"price\"] - quadratic_model.predict(val_df))**2))\n",
    "    rmse_cubic = np.sqrt(np.mean((val_df[\"price\"] - cubic_model.predict(val_df))**2))\n",
    "    rmse_quartic = np.sqrt(np.mean((val_df[\"price\"] - quartic_model.predict(val_df))**2))\n",
    "    \n",
    "    # store the results in a list\n",
    "    rmse_list.append([rmse_linear, rmse_quadratic, rmse_cubic, rmse_quartic])\n",
    "\n",
    "# Finally, average the model performance across the five folds.\n",
    "# If you repeatedly run this code, you'll find that the slope+intercept model\n",
    "# tends to perform best.\n",
    "rmse_df = pd.DataFrame(rmse_list, columns=['linear', 'quadratic', 'cubic', 'quartic'])\n",
    "rmse_df.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic model is still the best model. Interestingly, the cubic model is now way better than the quartic model (which was not the case in part (a)), but still worse than the linear and quadratic models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
